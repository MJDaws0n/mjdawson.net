Kernova — stealth sandbox for Windows applications

Elevator pitch

Kernova is a stealthy, high-fidelity sandbox for Windows apps: it runs programs inside a contained environment that looks like a normal Windows system from the program’s point of view, while Kernova captures every meaningful interaction — file writes, registry accesses, process activity, memory snapshots and network attempts — and keeps all side effects inside the sandbox. The goal is authentic behaviour from the target app plus forensic-grade observability for the analyst.

⸻

The problem Kernova solves

Lightweight sandboxes and compatibility layers often change the way a program behaves or make the program detect it’s being observed. Full virtual machines are heavy and hard to instrument. Kernova aims for a middle ground: realistic environment fidelity + deep observability + strong containment, so you can run suspicious or unknown software and see what it will actually do — while keeping the host safe.

⸻

What it does (user-level)
	•	Launches a Windows executable inside a contained runtime that mirrors the host environment.
	•	Routes file/registry operations into a sandbox image so changes never touch the real host.
	•	Logs and timestamps key runtime events: file I/O, process/thread lifecycle, module loads, and network attempts.
	•	Captures memory snapshots at configurable points for offline forensic analysis.
	•	Optionally simulates or proxies external services so the target thinks it’s communicating normally.
	•	Lets you replay runs, compare behaviours across configurations, and export structured reports.

⸻

Key benefits
	•	Realistic runs: targets see a believable environment, increasing the chance of authentic behaviour.
	•	Granular observability: detailed event timelines and memory captures help you understand what happened, when.
	•	Safety: side effects are contained and network interactions can be constrained.
	•	Repeatability: reproduce runs and compare differences under controlled changes.
	•	Practical UX: drag-and-drop launching, live timeline, and quick export/reporting.

⸻

Technical overview (high-level, conceptual)

Note: this is intentionally descriptive rather than instructional — it explains component roles and data flows without providing implementation steps or low-level tricks.

Main components
	1.	Launcher / Controller
	•	Orchestrates a run: config, environment snapshot, launch, monitoring, and shutdown.
	•	Stores metadata for each run (configuration, time, annotations).
	2.	Environment Mirror (Sandbox View)
	•	Presents a localized view of the filesystem, registry and visible services to the target process.
	•	The target interacts with this mirror as if it were the real system; all writes are redirected to a sandbox image.
	3.	Observation & Event Bus
	•	Central telemetry channel that collects events from instrumentation points: file operations, process events, module loads, and network attempts.
	•	Events are stored as structured records with timestamps and contextual metadata.
	4.	Memory Snapshotter
	•	Captures memory states of the instrumented process at configured points (start, periodic, end, or on-demand).
	•	Snapshots are catalogued and linked to the event timeline.
	5.	Network Gate / Proxy
	•	Controls outbound connections: allow, block, or proxy/masquerade responses to the target.
	•	Provides recorded or synthetic responses to coax behaviour without touching the Internet.
	6.	Replay & Comparison Engine
	•	Replays recorded event sequences or re-runs the target under a saved environment to compare differences.
	•	Produces diffs between runs to highlight divergent behaviour.
	7.	User Interface
	•	GUI showing active timeline, event filters, memory snapshot browser and export/report tools.
	•	Lightweight CLI for scripted runs and automation.

Data flows (conceptual)
	•	User config → Launcher starts run with an isolated environment snapshot.
	•	Instrumentation hooks generate events → Event Bus aggregates and timestamps them.
	•	File and registry calls are redirected into the sandbox image → logs record the virtual path and operation.
	•	Network calls hit Network Gate → proxy/simulated responses or block rules are applied → event logged.
	•	Memory Snapshotter writes snapshots to artifact storage, referenced by run metadata.
	•	After shutdown, all artifacts and the run timeline are available in the UI for review, export or replay.

Types of events captured (examples)
	•	File: open/read/write/delete/rename (logged with virtual path and result)
	•	Registry: key read/write/creation/deletion attempts
	•	Process: spawn, exit, injected modules, loaded DLLs
	•	Network: attempted connections, DNS requests, HTTP endpoints contacted
	•	Memory: snapshot markers, heap/allocation metadata (high-level summary), areas of interest
	•	Anomalies: crashes, access violations, suspicious file patterns (flagged, not acted upon)

Abstraction & fidelity strategy (conceptual)
	•	Keep the surface area the target sees as close to a real Windows host as feasible at the chosen fidelity level (filesystem layout, common registry keys, expected services).
	•	Route side-effecting operations into ephemeral or versioned images so each run can start from a known clean state.
	•	Use proxies or canned service responses to simulate network services rather than allowing live external communications.
	•	Instrumentation should be structured and minimally invasive so events are captured without dramatically altering observable timing or behavior (goal: authenticity).

⸻

Build plan & phased approach

This is a practical plan that delivers working functionality in incremental, testable steps. It’s phrased as a product-development roadmap rather than a how-to.

Phase 0 — Design & prototypes
	•	Define run configuration format and event schema.
	•	Mock UI flows and user journeys.
	•	Prototype a sandbox image format and basic run metadata store.

Phase 1 — MVP: filesystem-overlay + basic logging
	•	Launcher + environment mirror: create an overlay FS where writes are contained.
	•	Instrument file and process events and persist them to the event store.
	•	Basic GUI: load config, start run, show timeline of file/process events.

Phase 2 — Networking & snapshots
	•	Add network gate with allow/block/proxy rules and network event logging.
	•	Add memory snapshotting capability and linking into the timeline.
	•	Improve logging schema to support richer queries and exports.

Phase 3 — Replay and comparison
	•	Implement automatic replay of runs (re-run under saved environment) and timeline diffing.
	•	Add UI for side-by-side comparisons and highlighting of changed artefacts.

Phase 4 — UX polish & hardening
	•	UX improvements: filters, search, export templates, and report generator.
	•	Strengthen containment layers: artifact storage isolation, tamper-evident logs (conceptual).
	•	Add optional features: canned simulated services, run annotations, automatic heuristics for interesting events.

Stretch / exploratory
	•	Investigate low-level integration options for higher fidelity (driver/proxy) — treat as research, evaluate risk, and only pursue if containment is rock-solid.
	•	Add automation hooks for batch processing or curated sample sets.

⸻

MVP feature checklist
	•	Launch config + run metadata.
	•	Sandboxed filesystem overlay (writes go to sandbox image).
	•	Basic event logging: file and process lifecycle events.
	•	GUI timeline and event viewer.
	•	Exportable run logs and artifacts.
	•	Simple network rules (allow/block/proxy).

⸻

Demo plan (concise)
	1.	Show Kernova UI and a prepared run configuration.
	2.	Drag a benign test binary into Kernova and start the run.
	3.	Live timeline: highlight a file write, a spawned process and a network attempt.
	4.	Open a memory snapshot taken during the run and show how it links to the event time.
	5.	Re-run with the network blocked and show the behavioural diff in the timeline.
	6.	Export a run report.

⸻

Who should build it / required skills
	•	Systems-level programmer(s) comfortable with process management concepts and OS interfaces.
	•	Backend/data engineer for event schema, storage and artifact handling.
	•	Front-end/UI person to make the timeline and snapshot tools usable.
	•	Security-minded reviewer to validate containment and run scenarios.

⸻

Why Kernova is compelling

Kernova provides the sweet spot between authenticity and control: a sandbox that preserves the program’s normal view of a Windows host while giving the analyst full visibility and replayability. It’s ideal for forensic-style observation, behavioural research and safe, repeatable analysis when you want to understand what a program actually does in the wild.